{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parametrized Policy Network\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_space_dims, action_space_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_space1 = 16\n",
    "        hidden_space2 = 32\n",
    "\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(obs_space_dims,hidden_space1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_space1,hidden_space2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.policy_mean_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2,action_space_dims)\n",
    "        )\n",
    "\n",
    "        self.policy_stddev_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2,action_space_dims)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_features = self.shared_net(x.float())\n",
    "        action_means = self.policy_mean_net(shared_features)\n",
    "        action_stddevs = torch.log(\n",
    "            1+torch.exp(self.policy_stddev_net(shared_features))\n",
    "        )\n",
    "\n",
    "        return action_means, action_stddevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##REINFORCE\n",
    "class REINFOCE:\n",
    "\n",
    "    def __init__(self, obs_space_dims,action_space_dims):\n",
    "        self.learning_rate = 1e-4\n",
    "        self.gamma = 0.99\n",
    "        self.eps = 1e-6\n",
    "\n",
    "        self.net = PolicyNetwork(obs_space_dims, action_space_dims)\n",
    "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self.probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        state = torch.tensor(np.array([state]))\n",
    "        action_means, action_stddevs = self.net(state)\n",
    "\n",
    "        distrib = Normal(action_means[0]+self.eps, action_stddevs[0]+self.eps)\n",
    "        action = distrib.sample()\n",
    "        prob = distrib.log_prob(action)\n",
    "        \n",
    "        action = action.numpy()\n",
    "\n",
    "        self.probs.append(prob)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self):\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "\n",
    "        for R in self.rewards[::-1]:\n",
    "            running_g = R+self.gamma*running_g\n",
    "            gs.insert(0,running_g)\n",
    "\n",
    "        deltas = torch.tensor(gs)\n",
    "\n",
    "        loss = 0\n",
    "        for log_prob, delta in zip(self.probs, deltas):\n",
    "            loss += log_prob.mean()*delta*(-1)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.probs = []\n",
    "        self.rewards = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Average Reward: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\glfw\\__init__.py:916: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     22\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39msample_action(obs)\n\u001b[1;32m---> 23\u001b[0m     obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m wrapped_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     24\u001b[0m     agent\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     26\u001b[0m     done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\wrappers\\record_episode_statistics.py:94\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     87\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     (\n\u001b[0;32m     89\u001b[0m         observations,\n\u001b[0;32m     90\u001b[0m         rewards,\n\u001b[0;32m     91\u001b[0m         terminations,\n\u001b[0;32m     92\u001b[0m         truncations,\n\u001b[0;32m     93\u001b[0m         infos,\n\u001b[1;32m---> 94\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     95\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m     96\u001b[0m         infos, \u001b[39mdict\u001b[39m\n\u001b[0;32m     97\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`info` dtype is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(infos)\u001b[39m}\u001b[39;00m\u001b[39m while supported dtype is `dict`. This may be due to usage of other wrappers in the wrong order.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode_returns \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\envs\\mujoco\\inverted_pendulum_v4.py:123\u001b[0m, in \u001b[0;36mInvertedPendulumEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    121\u001b[0m terminated \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(\u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(ob)\u001b[39m.\u001b[39mall() \u001b[39mor\u001b[39;00m (np\u001b[39m.\u001b[39mabs(ob[\u001b[39m1\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0.2\u001b[39m))\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m ob, reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_env.py:409\u001b[0m, in \u001b[0;36mMujocoEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 409\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmujoco_renderer\u001b[39m.\u001b[39;49mrender(\n\u001b[0;32m    410\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcamera_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcamera_name\n\u001b[0;32m    411\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:673\u001b[0m, in \u001b[0;36mMujocoRenderer.render\u001b[1;34m(self, render_mode, camera_id, camera_name)\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n\u001b[0;32m    672\u001b[0m \u001b[39melif\u001b[39;00m render_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 673\u001b[0m     \u001b[39mreturn\u001b[39;00m viewer\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:402\u001b[0m, in \u001b[0;36mWindowViewer.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_paused:\n\u001b[0;32m    401\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_paused:\n\u001b[1;32m--> 402\u001b[0m         update()\n\u001b[0;32m    403\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_advance_by_one_step:\n\u001b[0;32m    404\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_advance_by_one_step \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:365\u001b[0m, in \u001b[0;36mWindowViewer.render.<locals>.update\u001b[1;34m()\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewport\u001b[39m.\u001b[39mwidth, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewport\u001b[39m.\u001b[39mheight \u001b[39m=\u001b[39m glfw\u001b[39m.\u001b[39mget_framebuffer_size(\n\u001b[0;32m    362\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow\n\u001b[0;32m    363\u001b[0m )\n\u001b[0;32m    364\u001b[0m \u001b[39m# update scene\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m mujoco\u001b[39m.\u001b[39;49mmjv_updateScene(\n\u001b[0;32m    366\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[0;32m    367\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvopt,\n\u001b[0;32m    369\u001b[0m     mujoco\u001b[39m.\u001b[39;49mMjvPerturb(),\n\u001b[0;32m    370\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcam,\n\u001b[0;32m    371\u001b[0m     mujoco\u001b[39m.\u001b[39;49mmjtCatBit\u001b[39m.\u001b[39;49mmjCAT_ALL\u001b[39m.\u001b[39;49mvalue,\n\u001b[0;32m    372\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscn,\n\u001b[0;32m    373\u001b[0m )\n\u001b[0;32m    375\u001b[0m \u001b[39m# marker items\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[39mfor\u001b[39;00m marker \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_markers:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"InvertedPendulum-v4\",render_mode=\"human\")\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env,50)\n",
    "\n",
    "total_num_episodes = int(5e3)\n",
    "obs_space_dims = env.observation_space.shape[0]\n",
    "action_space_dims = env.action_space.shape[0]\n",
    "rewards_over_seeds = []\n",
    "\n",
    "for seed in [1,2,3,4,5,8]:\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    agent = REINFOCE(obs_space_dims,action_space_dims)\n",
    "    reward_over_episodes=[]\n",
    "\n",
    "    for episode in range(total_num_episodes):\n",
    "        obs, info = wrapped_env.reset(seed=seed)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.sample_action(obs)\n",
    "            obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "            agent.rewards.append(reward)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "        reward_over_episodes.append(wrapped_env.return_queue[-1])\n",
    "        agent.update()\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            avg_reward = int(np.mean(wrapped_env.return_queue))\n",
    "            print(\"Episode:\", episode, \"Average Reward:\", avg_reward)\n",
    "\n",
    "    rewards_over_seeds.append(reward_over_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `reward` for parameter `y`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m df1\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mvariable\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mepisodes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mrewards\u001b[39m\u001b[39m\"\u001b[39m}, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m sns\u001b[39m.\u001b[39mset(style\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdarkgrid\u001b[39m\u001b[39m\"\u001b[39m,context\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtalk\u001b[39m\u001b[39m\"\u001b[39m, palette\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrainbow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m sns\u001b[39m.\u001b[39;49mlineplot(x\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepisodes\u001b[39;49m\u001b[39m\"\u001b[39;49m, y\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mreward\u001b[39;49m\u001b[39m\"\u001b[39;49m, data\u001b[39m=\u001b[39;49mdf1)\u001b[39m.\u001b[39mset(\n\u001b[0;32m      7\u001b[0m     title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mREINFORCE for InvertedPendulum-4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\seaborn\\relational.py:618\u001b[0m, in \u001b[0;36mlineplot\u001b[1;34m(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m errorbar \u001b[39m=\u001b[39m _deprecate_ci(errorbar, ci)\n\u001b[0;32m    617\u001b[0m variables \u001b[39m=\u001b[39m _LinePlotter\u001b[39m.\u001b[39mget_semantics(\u001b[39mlocals\u001b[39m())\n\u001b[1;32m--> 618\u001b[0m p \u001b[39m=\u001b[39m _LinePlotter(\n\u001b[0;32m    619\u001b[0m     data\u001b[39m=\u001b[39;49mdata, variables\u001b[39m=\u001b[39;49mvariables,\n\u001b[0;32m    620\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator, n_boot\u001b[39m=\u001b[39;49mn_boot, seed\u001b[39m=\u001b[39;49mseed, errorbar\u001b[39m=\u001b[39;49merrorbar,\n\u001b[0;32m    621\u001b[0m     sort\u001b[39m=\u001b[39;49msort, orient\u001b[39m=\u001b[39;49morient, err_style\u001b[39m=\u001b[39;49merr_style, err_kws\u001b[39m=\u001b[39;49merr_kws,\n\u001b[0;32m    622\u001b[0m     legend\u001b[39m=\u001b[39;49mlegend,\n\u001b[0;32m    623\u001b[0m )\n\u001b[0;32m    625\u001b[0m p\u001b[39m.\u001b[39mmap_hue(palette\u001b[39m=\u001b[39mpalette, order\u001b[39m=\u001b[39mhue_order, norm\u001b[39m=\u001b[39mhue_norm)\n\u001b[0;32m    626\u001b[0m p\u001b[39m.\u001b[39mmap_size(sizes\u001b[39m=\u001b[39msizes, order\u001b[39m=\u001b[39msize_order, norm\u001b[39m=\u001b[39msize_norm)\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\seaborn\\relational.py:365\u001b[0m, in \u001b[0;36m_LinePlotter.__init__\u001b[1;34m(self, data, variables, estimator, n_boot, seed, errorbar, sort, orient, err_style, err_kws, legend)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    352\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m,\n\u001b[0;32m    353\u001b[0m     data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, variables\u001b[39m=\u001b[39m{},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[39m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[0;32m    360\u001b[0m     \u001b[39m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_size_range \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         np\u001b[39m.\u001b[39mr_[\u001b[39m.5\u001b[39m, \u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m mpl\u001b[39m.\u001b[39mrcParams[\u001b[39m\"\u001b[39m\u001b[39mlines.linewidth\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    363\u001b[0m     )\n\u001b[1;32m--> 365\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, variables\u001b[39m=\u001b[39;49mvariables)\n\u001b[0;32m    367\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator \u001b[39m=\u001b[39m estimator\n\u001b[0;32m    368\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrorbar \u001b[39m=\u001b[39m errorbar\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\seaborn\\_oldcore.py:640\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[39m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[39m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[39m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[39m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_ordered \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m}  \u001b[39m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massign_variables(data, variables)\n\u001b[0;32m    642\u001b[0m \u001b[39mfor\u001b[39;00m var, \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_semantic_mappings\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    643\u001b[0m \n\u001b[0;32m    644\u001b[0m     \u001b[39m# Create the mapping function\u001b[39;00m\n\u001b[0;32m    645\u001b[0m     map_func \u001b[39m=\u001b[39m partial(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmap, plotter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\seaborn\\_oldcore.py:701\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_format \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 701\u001b[0m     plot_data, variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_variables_longform(\n\u001b[0;32m    702\u001b[0m         data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mvariables,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot_data \u001b[39m=\u001b[39m plot_data\n\u001b[0;32m    706\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables \u001b[39m=\u001b[39m variables\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\tf\\lib\\site-packages\\seaborn\\_oldcore.py:938\u001b[0m, in \u001b[0;36mVectorPlotter._assign_variables_longform\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(val, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m)):\n\u001b[0;32m    934\u001b[0m \n\u001b[0;32m    935\u001b[0m     \u001b[39m# This looks like a column name but we don't know what it means!\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     err \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not interpret value `\u001b[39m\u001b[39m{\u001b[39;00mval\u001b[39m}\u001b[39;00m\u001b[39m` for parameter `\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[0;32m    940\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m     \u001b[39m# Otherwise, assume the value is itself data\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \n\u001b[0;32m    944\u001b[0m     \u001b[39m# Raise when data object is present and a vector can't matched\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pd\u001b[39m.\u001b[39mDataFrame) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(val, pd\u001b[39m.\u001b[39mSeries):\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret value `reward` for parameter `y`"
     ]
    }
   ],
   "source": [
    "#Plot Learning Curve\n",
    "rewards_to_plot = [[reward[0] for reward in rewards] for rewards in rewards_over_seeds]\n",
    "df1 = pd.DataFrame(rewards_to_plot).melt()\n",
    "df1.rename(columns={\"variable\":\"episodes\", \"value\":\"rewards\"}, inplace=True)\n",
    "sns.set(style=\"darkgrid\",context=\"talk\", palette=\"rainbow\")\n",
    "sns.lineplot(x=\"episodes\", y=\"reward\", data=df1).set(\n",
    "    title=\"REINFORCE for InvertedPendulum-4\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
